{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 19 16:17:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.26                 Driver Version: 546.26       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P3              13W /  30W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 386.1 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-cpp-python==0.1.78) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-cpp-python==0.1.78) (1.26.0)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.5/45.5 kB 188.8 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [397 lines of output]\n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v144)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v144)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v144' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 17 2022\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v144' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v143' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 17 2022\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v143' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 16 2019 x64 v142' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 16 2019\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 16 2019 x64 v142' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 15 2017 x64 v141' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 15 2017\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 15 2017 x64 v141' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v144)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v144)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "                      ********************************************************************************\n",
      "                      scikit-build could not get a working generator for your system. Aborting build.\n",
      "      \n",
      "                      Building windows wheels requires Microsoft Visual Studio.\n",
      "                  Get it from:\n",
      "      n\n",
      "                   https://visualstudio.microsoft.com/vs/\n",
      "      \n",
      "                      ********************************************************************************\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python==0.1.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download / Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"second-state/All-MiniLM-L6-v2-Embedding-GGUF\"\n",
    "model_basename = \"all-MiniLM-L6-v2-Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b634dc26c75b496796351326e7b4468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all-MiniLM-L6-v2-Q4_K_M.gguf:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path, filename=model_basename, force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"C:\\Users\\User\\.cache\\huggingface\\hub\\models--second-state--All-MiniLM-L6-v2-Embedding-GGUF\\snapshots\\544f204f2eaa2d71361ffc74d6df7170285b286a\\all-MiniLM-L6-v2-Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\models--second-state--All-MiniLM-L6-v2-Embedding-GGUF\\\\snapshots\\\\544f204f2eaa2d71361ffc74d6df7170285b286a\\\\all-MiniLM-L6-v2-Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--second-state--All-MiniLM-L6-v2-Embedding-GGUF\\\\snapshots\\\\544f204f2eaa2d71361ffc74d6df7170285b286a\\\\all-MiniLM-L6-v2-Q4_K_M.gguf'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.path.exists(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_paths = []\n",
    "for root, dirs, files in os.walk(\"C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".gguf\"):  # replace with actual model extension\n",
    "            model_paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redownload CUDA 12.4 and full torch and install cuda to a simple folder path that does not have space, also first install llama-cpp-python on cpu, specify numpy 1.26.4 then upgrade to gpu \\\n",
    "Down grade llama-cpp::worked but no gpu support \\\n",
    "remember ggml_cuda args is only fro new models, use dllama for old models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\Meta-Llama-3-70B-Instruct.IQ2_XS.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from C:\\Users\\User\\.cache\\huggingface\\hub\\models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GGUF\\snapshots\\709de6cbe9635be085f64b9210bfebf75492b463\\Meta-Llama-3-70B-Instruct.IQ2_XS.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 20\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q2_K:   11 tensors\n",
      "llama_model_loader: - type q4_K:   80 tensors\n",
      "llama_model_loader: - type q5_K:    1 tensors\n",
      "llama_model_loader: - type iq2_xs:  470 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = IQ2_XS - 2.3125 bpw\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 19.68 GiB (2.40 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.55 MiB\n",
      "llm_load_tensors: offloading 15 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 15/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 20155.19 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3571.88 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   130.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    30.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    18.01 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   145.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   298.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.block_count': '80', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.head_count': '64', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '20', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Using chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=15, # Uncomment to use GPU acceleration\n",
    "    # seed=1337, # Uncomment to set a specific seed\n",
    "    # n_ctx=2048, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   42662.60 ms\n",
      "llama_print_timings:      sample time =      22.54 ms /    16 runs   (    1.41 ms per token,   709.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42661.77 ms /     6 tokens ( 7110.30 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:        eval time =  479269.43 ms /    15 runs   (31951.30 ms per token,     0.03 tokens per second)\n",
      "llama_print_timings:       total time =  522572.80 ms /    21 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-fa9a8ecb-e890-4c42-8aa8-b3f733e6f933',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1724602502,\n",
       " 'model': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GGUF\\\\snapshots\\\\709de6cbe9635be085f64b9210bfebf75492b463\\\\Meta-Llama-3-70B-Instruct.IQ2_XS.gguf',\n",
       " 'choices': [{'text': ' A noun is a word that refers to a person, place, thing, or',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 6, 'completion_tokens': 16, 'total_tokens': 22}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('what is a noun?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"D:\\AI Models\\huggingface\\models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GGUF\\snapshots\\709de6cbe9635be085f64b9210bfebf75492b463\\Meta-Llama-3-70B-Instruct.IQ2_XS.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = \"D:\\\\AI Models\\\\huggingface\\\\models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GGUF\\\\snapshots\\\\709de6cbe9635be085f64b9210bfebf75492b463\\\\Meta-Llama-3-70B-Instruct.IQ2_XS.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from D:\\AI Models\\huggingface\\models--MaziyarPanahi--Meta-Llama-3-70B-Instruct-GGUF\\snapshots\\709de6cbe9635be085f64b9210bfebf75492b463\\Meta-Llama-3-70B-Instruct.IQ2_XS.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 20\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q2_K:   11 tensors\n",
      "llama_model_loader: - type q4_K:   80 tensors\n",
      "llama_model_loader: - type q5_K:    1 tensors\n",
      "llama_model_loader: - type iq2_xs:  470 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = IQ2_XS - 2.3125 bpw\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 19.68 GiB (2.40 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 20155.19 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    18.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   266.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'hub', 'general.architecture': 'llama', 'llama.block_count': '80', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.attention.head_count': '64', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '20', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Using chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=drive_path,\n",
    "    # n_threads=4,  # CPU cores\n",
    "    # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    # n_batch=48,\n",
    "    # Change this value based on your model and your GPU VRAM pool.\n",
    "    # n_gpu_layers=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcpp_llm.model_params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcpp_llm('What is your name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression code\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9713.69 ms\n",
      "llama_print_timings:      sample time =     416.30 ms /   256 runs   (    1.63 ms per token,   614.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9713.15 ms /    30 tokens (  323.77 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:        eval time =  227732.04 ms /   255 runs   (  893.07 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time =  241846.98 ms /   285 tokens\n"
     ]
    }
   ],
   "source": [
    "response = lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                    repeat_penalty=1.2, top_k=150,\n",
    "                    echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-b52d6965-1f14-4ecd-a73e-293f4568283e', 'object': 'text_completion', 'created': 1724292863, 'model': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf', 'choices': [{'text': 'SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Write a linear regression code\\n\\nASSISTANT:\\nI can do that.\\n```\\nThe above is the expected output for this case where I am trying to get my user to write some simple linear regression codes using scikit-learn . The system will respond with an appropriate response when it detects keywords in the users\\' input.\\n\\n## How does RASA work?\\n\\nRasa NLU uses a combination of machine learning and natural language processing (NLP) techniques to understand text. It can be used for any type of dialogue, from simple questions about your company\\'s products or services up until complex conversations that require multiple steps before they are resolved.\\n\\nThe system first tries to match the user input with an intent using spacy . If it does not find a matching intent in its database then Rasa will try again but this time looking for entities instead. After finding all possible matches, if there is still no clear winner among them (i.e., multiple intents share similar features), they are combined into one single entity called \"None\" which means that none of the other candidates were strong enough to win over each other.\\n\\n## How does RASA work?\\n\\nRasa uses a combination of machine learning and natural language processing techniques to understand text. It can be used for any type of dialogue, from simple questions about your company’s products or services', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 30, 'completion_tokens': 256, 'total_tokens': 286}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a linear regression code\n",
      "\n",
      "ASSISTANT:\n",
      "I can do that.\n",
      "```\n",
      "The above is the expected output for this case where I am trying to get my user to write some simple linear regression codes using scikit-learn . The system will respond with an appropriate response when it detects keywords in the users' input.\n",
      "\n",
      "## How does RASA work?\n",
      "\n",
      "Rasa NLU uses a combination of machine learning and natural language processing (NLP) techniques to understand text. It can be used for any type of dialogue, from simple questions about your company's products or services up until complex conversations that require multiple steps before they are resolved.\n",
      "\n",
      "The system first tries to match the user input with an intent using spacy . If it does not find a matching intent in its database then Rasa will try again but this time looking for entities instead. After finding all possible matches, if there is still no clear winner among them (i.e., multiple intents share similar features), they are combined into one single entity called \"None\" which means that none of the other candidates were strong enough to win over each other.\n",
      "\n",
      "## How does RASA work?\n",
      "\n",
      "Rasa uses a combination of machine learning and natural language processing techniques to understand text. It can be used for any type of dialogue, from simple questions about your company’s products or services\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'what would be a good name for a company that makes colorful socks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 12 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   13323.91 ms\n",
      "llama_print_timings:      sample time =      19.46 ms /    16 runs   (    1.22 ms per token,   822.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =  149918.73 ms /    16 runs   ( 9369.92 ms per token,     0.11 tokens per second)\n",
      "llama_print_timings:       total time =  150328.45 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\". You can find the answer to this question below:\n",
      "What is the best\n"
     ]
    }
   ],
   "source": [
    "print(lcpp_llm(prompt)['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = 'I wan to open a restaurant for nigerian foods, suggest me fance names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 12 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   13323.91 ms\n",
      "llama_print_timings:      sample time =       8.50 ms /    16 runs   (    0.53 ms per token,  1881.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =   29667.58 ms /    16 runs   ( 1854.22 ms per token,     0.54 tokens per second)\n",
      "llama_print_timings:       total time =   30353.12 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-069e3a72-2073-44d7-a585-695cdcebbbc4', 'object': 'text_completion', 'created': 1724260743, 'model': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf', 'choices': [{'text': ' for men?\\nwhat is the name of the company that makes socks for men?\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 16, 'total_tokens': 29}}\n"
     ]
    }
   ],
   "source": [
    "print(lcpp_llm(prompt2)['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1=PromptTemplate(input_variables=['cuisine'], template=\"i want to open a restaurant for {cuisine} food, suggest a fancy name for me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = prompt_template1.format(cuisine=\"indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to open a restaurant for indian food, suggest a fancy name for me\n"
     ]
    }
   ],
   "source": [
    "print(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = PromptTemplate(input_variables=[\"book_naem\"], template='Provide me a concise summary of the book {book_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt2 = prompt_template2.format(book_name='Alchemist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide me a concise summary of the book Alchemist\n"
     ]
    }
   ],
   "source": [
    "print(input_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:140\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Llama' from 'llama_cpp' (unknown location)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf16_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallbackManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStreamingStdOutCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\v1\\main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:142\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m model_path \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    149\u001b[0m model_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m ]\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_threads=4,\n",
    "    n_gpu_layers=20,\n",
    "    n_batch=48,\n",
    "    n_ctx=1024,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I will give the definition by Prof. Yitang Zhang as an example.\n",
      "Let us recall that in 2013, Prof. Zhang announced his proof of the bounded gap problem. Since then, there have been many papers and comments on this subject. Among them is a survey paper published in May 2016 by Dr. Fan Chung Graham et al. In this paper, it was stated: \"In particular, Zhang's work is motivated by the circle method (or the Hardy-Littlewood method), which originated from works of Hardy and Littlewood.\" The authors also pointed out that: \"The work of Yitang Zhang, announced on May 27th, 2013 has been accepted for publication in the journal Annals of Mathematics. The proof was reviewed by a committee consisting of Professors Enrico Bombieri (chair), Andrew Granville and Peter Sarnak.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7355.02 ms\n",
      "llama_print_timings:      sample time =     211.00 ms /   180 runs   (    1.17 ms per token,   853.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7353.74 ms /     6 tokens ( 1225.62 ms per token,     0.82 tokens per second)\n",
      "llama_print_timings:        eval time =  409760.50 ms /   179 runs   ( 2289.16 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time =  421983.84 ms /   185 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I will give the definition by Prof. Yitang Zhang as an example.\\nLet us recall that in 2013, Prof. Zhang announced his proof of the bounded gap problem. Since then, there have been many papers and comments on this subject. Among them is a survey paper published in May 2016 by Dr. Fan Chung Graham et al. In this paper, it was stated: \"In particular, Zhang\\'s work is motivated by the circle method (or the Hardy-Littlewood method), which originated from works of Hardy and Littlewood.\" The authors also pointed out that: \"The work of Yitang Zhang, announced on May 27th, 2013 has been accepted for publication in the journal Annals of Mathematics. The proof was reviewed by a committee consisting of Professors Enrico Bombieri (chair), Andrew Granville and Peter Sarnak.\"'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('How can you define mathematics?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='i want to open a restaurant for {cuisine} food, suggest a fancy name for me')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=prompt_template1,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp),\n",
    "                   prompt_template1)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Thanks for the A2A question! You can see some of my previous answers here. If you have any questions about my work or how to get started in this industry, feel free to ask away!\n",
      "What is the process to start a food business?\n",
      "The process to start a food business varies depending on what type of food service you want to provide. For example, if you’re opening a restaurant then there will be additional steps that need to be taken compared with starting a catering company or running an ice cream truck.\n",
      "Some examples include but are not limited:\n",
      "Obtaining a Business License\n",
      "The first step is to obtain a business license from the local government office in your area. This can usually be done through a website and involves filling out some basic information about yourself and your proposed business venture. Once you’ve submitted this application, it will then need to go through an approval process that takes into account things like potential competition within the industry and any other relevant factors.\n",
      "If you want to open a restaurant for Indian food then I suggest you use a name that is easy to remember and sounds professional. For example:\n",
      "Saffron Spice\n",
      "These are just some examples but there are many more out there so feel free to get creative when it comes time for choosing your own personal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10727.00 ms\n",
      "llama_print_timings:      sample time =     373.71 ms /   256 runs   (    1.46 ms per token,   685.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39813.31 ms /    47 tokens (  847.09 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:        eval time =  411370.37 ms /   255 runs   ( 1613.22 ms per token,     0.62 tokens per second)\n",
      "llama_print_timings:       total time =  432781.00 ms /   302 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\nThanks for the A2A question! You can see some of my previous answers here. If you have any questions about my work or how to get started in this industry, feel free to ask away!\\nWhat is the process to start a food business?\\nThe process to start a food business varies depending on what type of food service you want to provide. For example, if you’re opening a restaurant then there will be additional steps that need to be taken compared with starting a catering company or running an ice cream truck.\\nSome examples include but are not limited:\\nObtaining a Business License\\nThe first step is to obtain a business license from the local government office in your area. This can usually be done through a website and involves filling out some basic information about yourself and your proposed business venture. Once you’ve submitted this application, it will then need to go through an approval process that takes into account things like potential competition within the industry and any other relevant factors.\\nIf you want to open a restaurant for Indian food then I suggest you use a name that is easy to remember and sounds professional. For example:\\nSaffron Spice\\nThese are just some examples but there are many more out there so feel free to get creative when it comes time for choosing your own personal'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "cuisine = \"India\"\n",
    "chain.invoke({\"cuisine\": cuisine})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
