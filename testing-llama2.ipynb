{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 19 16:17:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.26                 Driver Version: 546.26       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P3              13W /  30W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 386.1 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-cpp-python==0.1.78) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from llama-cpp-python==0.1.78) (1.26.0)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.5/45.5 kB 188.8 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [397 lines of output]\n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v144)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v144)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v144' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 17 2022\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v144' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v143' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 17 2022\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 17 2022 x64 v143' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 16 2019 x64 v142' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 16 2019\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 16 2019 x64 v142' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      -- The C compiler identification is unknown\n",
      "      CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "        No CMAKE_C_COMPILER could be found.\n",
      "      \n",
      "        Tell CMake where to find the compiler by setting either the environment\n",
      "        variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "        the compiler, or to the compiler name if it is in the PATH.\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'Visual Studio 15 2017 x64 v141' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Generator\n",
      "      \n",
      "          Visual Studio 15 2017\n",
      "      \n",
      "        could not find any instance of Visual Studio.\n",
      "      \n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'Visual Studio 15 2017 x64 v141' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v144)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v144)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "      \n",
      "      \n",
      "      --------------------------------------------------------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator\n",
      "      --------------------------------\n",
      "      ---------------------------\n",
      "      ----------------------\n",
      "      -----------------\n",
      "      ------------\n",
      "      -------\n",
      "      --\n",
      "      Not searching for unused variables given on the command line.\n",
      "      CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "        Running\n",
      "      \n",
      "         'nmake' '-?'\n",
      "      \n",
      "        failed with:\n",
      "      \n",
      "         no such file or directory\n",
      "      \n",
      "      \n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      --\n",
      "      -------\n",
      "      ------------\n",
      "      -----------------\n",
      "      ----------------------\n",
      "      ---------------------------\n",
      "      --------------------------------\n",
      "      -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator - failure\n",
      "      --------------------------------------------------------------------------------\n",
      "      \n",
      "                      ********************************************************************************\n",
      "                      scikit-build could not get a working generator for your system. Aborting build.\n",
      "      \n",
      "                      Building windows wheels requires Microsoft Visual Studio.\n",
      "                  Get it from:\n",
      "      n\n",
      "                   https://visualstudio.microsoft.com/vs/\n",
      "      \n",
      "                      ********************************************************************************\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python==0.1.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download / Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"QuantFactory/Meta-Llama-3.1-8B-GGUF\"\n",
    "model_basename = \"Meta-Llama-3.1-8B.Q4_1.gguf\"  # the model is in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0153fb8ae35c46c085e0bbb0e82d4a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Meta-Llama-3.1-8B.Q4_1.gguf:   0%|          | 0.00/5.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path, filename=model_basename, force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from C:\\Users\\User\\.cache\\huggingface\\hub\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\snapshots\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\Meta-Llama-3.1-8B.Q4_1.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   5:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 3\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_1:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_1\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.77 GiB (5.10 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4885.12 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 48\n",
      "llama_new_context_with_model: n_ubatch   = 48\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    24.23 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Models', 'general.architecture': 'llama', 'general.type': 'model', 'general.size_label': '8.0B', 'general.license': 'llama3.1', 'llama.block_count': '32', 'llama.context_length': '131072', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '3', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'smaug-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=4,  # CPU cores\n",
    "    # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_batch=48,\n",
    "    # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_gpu_layers=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcpp_llm.model_params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression code\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9713.69 ms\n",
      "llama_print_timings:      sample time =     416.30 ms /   256 runs   (    1.63 ms per token,   614.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9713.15 ms /    30 tokens (  323.77 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:        eval time =  227732.04 ms /   255 runs   (  893.07 ms per token,     1.12 tokens per second)\n",
      "llama_print_timings:       total time =  241846.98 ms /   285 tokens\n"
     ]
    }
   ],
   "source": [
    "response = lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                    repeat_penalty=1.2, top_k=150,\n",
    "                    echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-b52d6965-1f14-4ecd-a73e-293f4568283e', 'object': 'text_completion', 'created': 1724292863, 'model': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf', 'choices': [{'text': 'SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Write a linear regression code\\n\\nASSISTANT:\\nI can do that.\\n```\\nThe above is the expected output for this case where I am trying to get my user to write some simple linear regression codes using scikit-learn . The system will respond with an appropriate response when it detects keywords in the users\\' input.\\n\\n## How does RASA work?\\n\\nRasa NLU uses a combination of machine learning and natural language processing (NLP) techniques to understand text. It can be used for any type of dialogue, from simple questions about your company\\'s products or services up until complex conversations that require multiple steps before they are resolved.\\n\\nThe system first tries to match the user input with an intent using spacy . If it does not find a matching intent in its database then Rasa will try again but this time looking for entities instead. After finding all possible matches, if there is still no clear winner among them (i.e., multiple intents share similar features), they are combined into one single entity called \"None\" which means that none of the other candidates were strong enough to win over each other.\\n\\n## How does RASA work?\\n\\nRasa uses a combination of machine learning and natural language processing techniques to understand text. It can be used for any type of dialogue, from simple questions about your company’s products or services', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 30, 'completion_tokens': 256, 'total_tokens': 286}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a linear regression code\n",
      "\n",
      "ASSISTANT:\n",
      "I can do that.\n",
      "```\n",
      "The above is the expected output for this case where I am trying to get my user to write some simple linear regression codes using scikit-learn . The system will respond with an appropriate response when it detects keywords in the users' input.\n",
      "\n",
      "## How does RASA work?\n",
      "\n",
      "Rasa NLU uses a combination of machine learning and natural language processing (NLP) techniques to understand text. It can be used for any type of dialogue, from simple questions about your company's products or services up until complex conversations that require multiple steps before they are resolved.\n",
      "\n",
      "The system first tries to match the user input with an intent using spacy . If it does not find a matching intent in its database then Rasa will try again but this time looking for entities instead. After finding all possible matches, if there is still no clear winner among them (i.e., multiple intents share similar features), they are combined into one single entity called \"None\" which means that none of the other candidates were strong enough to win over each other.\n",
      "\n",
      "## How does RASA work?\n",
      "\n",
      "Rasa uses a combination of machine learning and natural language processing techniques to understand text. It can be used for any type of dialogue, from simple questions about your company’s products or services\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'what would be a good name for a company that makes colorful socks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 12 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   13323.91 ms\n",
      "llama_print_timings:      sample time =      19.46 ms /    16 runs   (    1.22 ms per token,   822.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =  149918.73 ms /    16 runs   ( 9369.92 ms per token,     0.11 tokens per second)\n",
      "llama_print_timings:       total time =  150328.45 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\". You can find the answer to this question below:\n",
      "What is the best\n"
     ]
    }
   ],
   "source": [
    "print(lcpp_llm(prompt)['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = 'I wan to open a restaurant for nigerian foods, suggest me fance names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 12 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   13323.91 ms\n",
      "llama_print_timings:      sample time =       8.50 ms /    16 runs   (    0.53 ms per token,  1881.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (-nan(ind) ms per token, -nan(ind) tokens per second)\n",
      "llama_print_timings:        eval time =   29667.58 ms /    16 runs   ( 1854.22 ms per token,     0.54 tokens per second)\n",
      "llama_print_timings:       total time =   30353.12 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-069e3a72-2073-44d7-a585-695cdcebbbc4', 'object': 'text_completion', 'created': 1724260743, 'model': 'C:\\\\Users\\\\User\\\\.cache\\\\huggingface\\\\hub\\\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\\\snapshots\\\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\\\Meta-Llama-3.1-8B.Q4_1.gguf', 'choices': [{'text': ' for men?\\nwhat is the name of the company that makes socks for men?\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 16, 'total_tokens': 29}}\n"
     ]
    }
   ],
   "source": [
    "print(lcpp_llm(prompt2)['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1=PromptTemplate(input_variables=['cuisine'], template=\"i want to open a restaurant for {cuisine} food, suggest a fancy name for me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = prompt_template1.format(cuisine=\"indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to open a restaurant for indian food, suggest a fancy name for me\n"
     ]
    }
   ],
   "source": [
    "print(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = PromptTemplate(input_variables=[\"book_naem\"], template='Provide me a concise summary of the book {book_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt2 = prompt_template2.format(book_name='Alchemist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide me a concise summary of the book Alchemist\n"
     ]
    }
   ],
   "source": [
    "print(input_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from C:\\Users\\User\\.cache\\huggingface\\hub\\models--QuantFactory--Meta-Llama-3.1-8B-GGUF\\snapshots\\d0a93b5ad9c03e2e0f43b0814a36892638bfc856\\Meta-Llama-3.1-8B.Q4_1.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
      "llama_model_loader: - kv   4:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   5:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 3\n",
      "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_1:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_1\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.77 GiB (5.10 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4885.12 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 48\n",
      "llama_new_context_with_model: n_ubatch   = 48\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    24.23 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Models', 'general.architecture': 'llama', 'general.type': 'model', 'general.size_label': '8.0B', 'general.license': 'llama3.1', 'llama.block_count': '32', 'llama.context_length': '131072', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '3', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'smaug-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_threads=4,\n",
    "    n_gpu_layers=20,\n",
    "    n_batch=48,\n",
    "    n_ctx=1024,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I will give the definition by Prof. Yitang Zhang as an example.\n",
      "Let us recall that in 2013, Prof. Zhang announced his proof of the bounded gap problem. Since then, there have been many papers and comments on this subject. Among them is a survey paper published in May 2016 by Dr. Fan Chung Graham et al. In this paper, it was stated: \"In particular, Zhang's work is motivated by the circle method (or the Hardy-Littlewood method), which originated from works of Hardy and Littlewood.\" The authors also pointed out that: \"The work of Yitang Zhang, announced on May 27th, 2013 has been accepted for publication in the journal Annals of Mathematics. The proof was reviewed by a committee consisting of Professors Enrico Bombieri (chair), Andrew Granville and Peter Sarnak.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7355.02 ms\n",
      "llama_print_timings:      sample time =     211.00 ms /   180 runs   (    1.17 ms per token,   853.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7353.74 ms /     6 tokens ( 1225.62 ms per token,     0.82 tokens per second)\n",
      "llama_print_timings:        eval time =  409760.50 ms /   179 runs   ( 2289.16 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time =  421983.84 ms /   185 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I will give the definition by Prof. Yitang Zhang as an example.\\nLet us recall that in 2013, Prof. Zhang announced his proof of the bounded gap problem. Since then, there have been many papers and comments on this subject. Among them is a survey paper published in May 2016 by Dr. Fan Chung Graham et al. In this paper, it was stated: \"In particular, Zhang\\'s work is motivated by the circle method (or the Hardy-Littlewood method), which originated from works of Hardy and Littlewood.\" The authors also pointed out that: \"The work of Yitang Zhang, announced on May 27th, 2013 has been accepted for publication in the journal Annals of Mathematics. The proof was reviewed by a committee consisting of Professors Enrico Bombieri (chair), Andrew Granville and Peter Sarnak.\"'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('How can you define mathematics?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='i want to open a restaurant for {cuisine} food, suggest a fancy name for me')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=prompt_template1,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp),\n",
    "                   prompt_template1)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Thanks for the A2A question! You can see some of my previous answers here. If you have any questions about my work or how to get started in this industry, feel free to ask away!\n",
      "What is the process to start a food business?\n",
      "The process to start a food business varies depending on what type of food service you want to provide. For example, if you’re opening a restaurant then there will be additional steps that need to be taken compared with starting a catering company or running an ice cream truck.\n",
      "Some examples include but are not limited:\n",
      "Obtaining a Business License\n",
      "The first step is to obtain a business license from the local government office in your area. This can usually be done through a website and involves filling out some basic information about yourself and your proposed business venture. Once you’ve submitted this application, it will then need to go through an approval process that takes into account things like potential competition within the industry and any other relevant factors.\n",
      "If you want to open a restaurant for Indian food then I suggest you use a name that is easy to remember and sounds professional. For example:\n",
      "Saffron Spice\n",
      "These are just some examples but there are many more out there so feel free to get creative when it comes time for choosing your own personal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10727.00 ms\n",
      "llama_print_timings:      sample time =     373.71 ms /   256 runs   (    1.46 ms per token,   685.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39813.31 ms /    47 tokens (  847.09 ms per token,     1.18 tokens per second)\n",
      "llama_print_timings:        eval time =  411370.37 ms /   255 runs   ( 1613.22 ms per token,     0.62 tokens per second)\n",
      "llama_print_timings:       total time =  432781.00 ms /   302 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\nThanks for the A2A question! You can see some of my previous answers here. If you have any questions about my work or how to get started in this industry, feel free to ask away!\\nWhat is the process to start a food business?\\nThe process to start a food business varies depending on what type of food service you want to provide. For example, if you’re opening a restaurant then there will be additional steps that need to be taken compared with starting a catering company or running an ice cream truck.\\nSome examples include but are not limited:\\nObtaining a Business License\\nThe first step is to obtain a business license from the local government office in your area. This can usually be done through a website and involves filling out some basic information about yourself and your proposed business venture. Once you’ve submitted this application, it will then need to go through an approval process that takes into account things like potential competition within the industry and any other relevant factors.\\nIf you want to open a restaurant for Indian food then I suggest you use a name that is easy to remember and sounds professional. For example:\\nSaffron Spice\\nThese are just some examples but there are many more out there so feel free to get creative when it comes time for choosing your own personal'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "cuisine = \"India\"\n",
    "chain.invoke({\"cuisine\": cuisine})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
